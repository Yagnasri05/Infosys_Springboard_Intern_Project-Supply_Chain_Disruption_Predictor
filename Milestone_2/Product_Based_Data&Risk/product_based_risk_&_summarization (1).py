# -*- coding: utf-8 -*-
"""Product based Risk & Summarization.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZYKa_HbT_rH6B-bc7V3uBXFQS9DCQU-R

# Data Collection
"""

import requests
import pandas as pd
import re
import spacy

# Load spaCy's pre-trained NER model
nlp = spacy.load("en_core_web_sm")

# Define your Event Registry API key
api_key = "a6e45a4f-616a-46b2-ab77-e7b54c6435cc"

# Define the base URL for querying the Event Registry API
url = "https://eventregistry.org/api/v1/article/getArticles"

# Keyword for product-specific search
keywords = ["semiconductors", "semiconductor supply chain", "semiconductor logistics delay",
            "semiconductor strike", "semiconductor transportation", "semiconductor disruption"]

# Function to extract meaningful numerical data and reasons for disruptions
def extract_disruption_info(text):
    if not text:
        return {"Delays": "N/A", "Reason": "N/A", "Duration": "N/A"}

    # Extract delays (e.g., "5 days late")
    delays = re.findall(r"(\d+)\s+(days?|hours?|weeks?)", text)
    delay_str = ", ".join([f"{d[0]} {d[1]}" for d in delays]) if delays else "N/A"

    # Extract reasons (e.g., "strike," "transportation delay")
    reasons = re.findall(r"(strike|delay|shortage|logistics issue|transportation issue|risk)", text, re.IGNORECASE)
    reason_str = ", ".join(set(reasons)).capitalize() if reasons else "N/A"

    # Extract approximate durations (e.g., "for 2 weeks")
    durations = re.findall(r"for\s+(\d+)\s+(days?|weeks?|months?)", text, re.IGNORECASE)
    duration_str = ", ".join([f"{d[0]} {d[1]}" for d in durations]) if durations else "N/A"

    return {"Delays": delay_str, "Reason": reason_str, "Duration": duration_str}

# Function to extract locations using spaCy NER
def extract_locations(text):
    doc = nlp(text)
    locations = set([ent.text for ent in doc.ents if ent.label_ == 'GPE'])  # GPE = Geopolitical entity (countries, cities)
    return ", ".join(locations) if locations else "Unknown"

# Initialize an empty list to store all extracted data
all_data = []

# Number of articles per page
articles_per_page = 200
total_articles_to_fetch = 1000

# Iterate through each keyword to fetch articles
for keyword in keywords:
    print(f"Fetching articles for keyword: {keyword}")
    fetched_articles = 0  # Counter for the total number of articles fetched for the keyword

    while fetched_articles < total_articles_to_fetch:
        # Set up the query parameters
        params = {
            "apiKey": api_key,
            "keyword": keyword,
            "count": articles_per_page,
            "offset": fetched_articles,  # Offset for pagination
            "lang": "eng",  # Language of articles (English)
            "sortBy": "date",  # Sort by date (most recent first)
            "startDate": "2023-01-01",  # Start date for events
            "endDate": "2024-12-31",  # End date for events
        }

        # Send GET request to the API
        response = requests.get(url, params=params)

        # Check if the response is successful (status code 200)
        if response.status_code == 200:
            data = response.json()  # Convert the response to JSON
        else:
            print(f"Error: {response.status_code}")
            break

        # Check if articles are in the response
        if 'articles' in data and 'results' in data['articles']:
            articles = data['articles']['results']

            # Loop through each article to extract details
            for article in articles:
                summary = article.get('body', 'No content available')

                # Extract disruption details
                disruption_info = extract_disruption_info(summary)

                # Extract location information
                location = extract_locations(summary)

                # Append the extracted data to the list
                all_data.append({
                    'Keyword': keyword,
                    'Title': article.get('title', 'No Title'),
                    'Source': article.get('source', {}).get('title', 'No Source'),
                    'URL': article.get('url', 'No URL'),
                    'Summary': summary,
                    'Location': location,
                    'Delays': disruption_info["Delays"],
                    'Reason': disruption_info["Reason"],
                    'Duration': disruption_info["Duration"]
                })

            # Update the fetched articles count
            fetched_articles += len(articles)

            # Stop if we've fetched enough articles
            if len(all_data) >= total_articles_to_fetch:
                break
        else:
            print(f"No more articles found for keyword: {keyword}")
            break

# Convert the list of extracted data to a DataFrame
df = pd.DataFrame(all_data)

# Limit to 1000 rows if needed
df = df.head(1000)

# Save the data to a CSV file
output_path = '/content/drive/MyDrive/semiconductor_supply_chain_data.csv'
df.to_csv(output_path, index=False)
print(f"Data has been saved to '{output_path}'")

"""# RISK Factorization - Only Textual Data"""

import pandas as pd
from transformers import pipeline
from sklearn.preprocessing import MinMaxScaler
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np

# Load dataset
data_path = '/content/drive/MyDrive/Product_merged_supply_chain_data.csv'
df = pd.read_csv(data_path)

# Load pre-trained Hugging Face sentiment analysis model
sentiment_pipeline = pipeline("sentiment-analysis", model="distilbert-base-uncased")

# Generate textual risk factor based on sentiment analysis
def calculate_textual_risk(summary):
    sentiment = sentiment_pipeline(summary[:512])  # Limit summary to 512 characters
    sentiment_score = sentiment[0]['score']
    sentiment_label = sentiment[0]['label']
    textual_risk = -sentiment_score if sentiment_label == "NEGATIVE" else sentiment_score
    return textual_risk

# Apply textual risk factor calculation
df['Textual Risk'] = df['Summary'].apply(calculate_textual_risk)

# Determine dynamic thresholds for risk labels
min_risk = df['Textual Risk'].min()
max_risk = df['Textual Risk'].max()
threshold_1 = min_risk + (max_risk - min_risk) / 3  # Lower threshold
threshold_2 = min_risk + 2 * (max_risk - min_risk) / 3  # Upper threshold

# Assign Risk Labels
def assign_risk_label(risk_factor):
    if risk_factor >= threshold_2:
        return 'High'
    elif risk_factor >= threshold_1:
        return 'Medium'
    else:
        return 'Low'

df['Risk Label'] = df['Textual Risk'].apply(assign_risk_label)

# Save the updated dataset
df.to_csv('/content/drive/MyDrive/updated_risk_factorized_data1.csv', index=False)
print("Risk factorized data saved to '/content/drive/MyDrive/updated_risk_factorized_data.csv1'.")

# Display thresholds for reference
print(f"Risk thresholds:")
print(f"- Low: < {threshold_1}")
print(f"- Medium: ≥ {threshold_1} and < {threshold_2}")
print(f"- High: ≥ {threshold_2}")


# Vectorize the 'Summary' column using TF-IDF
tfidf_vectorizer = TfidfVectorizer(stop_words='english')
tfidf_matrix = tfidf_vectorizer.fit_transform(df['Summary'].fillna(''))

# Create a query-based summarization function
def query_summary(query_type, column_name=None):
    """
    Summarize insights from numerical or textual columns.
    - query_type: "average", "maximum", "minimum", "top_terms"
    - column_name: The column to summarize (for numerical queries)
    """
    if query_type.lower() in ['average', 'maximum', 'minimum']:
        # Handle numerical summarization
        if column_name in df.columns:
            if query_type.lower() == "average":
                result = df[column_name].mean()
            elif query_type.lower() == "maximum":
                result = df[column_name].max()
            elif query_type.lower() == "minimum":
                result = df[column_name].min()
            else:
                raise ValueError("Invalid query type for numerical summarization.")
            print(f"The {query_type} value for '{column_name}' is: {result}")
            return result
        else:
            raise ValueError(f"Column '{column_name}' not found in the dataset.")
    elif query_type.lower() == "top_terms":
        # Handle textual summarization for top terms
        feature_names = tfidf_vectorizer.get_feature_names_out()
        avg_tfidf_scores = np.mean(tfidf_matrix.toarray(), axis=0)
        top_indices = avg_tfidf_scores.argsort()[-10:][::-1]  # Top 10 terms
        top_terms = [(feature_names[i], avg_tfidf_scores[i]) for i in top_indices]
        print("Top terms by TF-IDF scores:")
        for term, score in top_terms:
            print(f"{term}: {score:.4f}")
        return top_terms
    else:
        raise ValueError("Invalid query type. Supported types: 'average', 'maximum', 'minimum', 'top_terms'.")


# Summarizing numerical insights
query_summary("average", "Textual Risk")
query_summary("maximum", "Textual Risk")
query_summary("minimum", "Textual Risk")

# Summarizing textual insights (Top Terms)
query_summary("top_terms")

"""# Risk Factorization & Summarization Product Based"""

import pandas as pd
from transformers import pipeline
from sklearn.preprocessing import MinMaxScaler
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np

# Load dataset
data_path = "/content/drive/MyDrive/Product_merged_supply_chain_data.csv"
df = pd.read_csv(data_path)

# Numerical columns for normalization
numerical_cols = ['Inventory Level', 'Lead Time (days)', 'News Sentiment', 'Risk Factor']

# Normalize numerical columns
scaler = MinMaxScaler()
df[numerical_cols] = scaler.fit_transform(df[numerical_cols])

# Load pre-trained Hugging Face sentiment model
sentiment_pipeline = pipeline("sentiment-analysis", model="distilbert-base-uncased")

# Generate textual risk factor based on sentiment analysis
def calculate_textual_risk(summary):
    sentiment = sentiment_pipeline(summary[:512])  # Limit summary to 512 characters
    sentiment_score = sentiment[0]['score']
    sentiment_label = sentiment[0]['label']
    textual_risk = -sentiment_score if sentiment_label == "NEGATIVE" else sentiment_score
    return textual_risk

# Apply textual risk factor calculation
df['Textual Risk'] = df['Summary'].apply(calculate_textual_risk)

# Combine numerical and textual data for final risk factor
df['Computed Risk Factor'] = df.apply(
    lambda row: (0.5 * row['Textual Risk'] +
                 0.2 * row['Inventory Level'] +
                 0.2 * row['Lead Time (days)'] +
                 0.1 * row['News Sentiment']), axis=1
)

min_risk = df['Computed Risk Factor'].min()
max_risk = df['Computed Risk Factor'].max()
threshold_1 = min_risk + (max_risk - min_risk) / 3  # Lower threshold
threshold_2 = min_risk + 2 * (max_risk - min_risk) / 3  # Upper threshold

# Assign Risk Labels
def assign_risk_label(risk_factor):
    if risk_factor >= threshold_2:
        return 'High'
    elif risk_factor >= threshold_1:
        return 'Medium'
    else:
        return 'Low'

df['Risk Label'] = df['Computed Risk Factor'].apply(assign_risk_label)

# Save the updated dataset
df.to_csv("/content/drive/MyDrive/Product_risk_factorized_data.csv", index=False)
print("Risk factorized data saved to '/content/drive/MyDrive/Product_risk_factorized_data.csv'.")

# Display thresholds for reference
print(f"Dynamic Risk Thresholds:")
print(f"- Low Risk: < {threshold_1:.4f}")
print(f"- Medium Risk: ≥ {threshold_1:.4f} and < {threshold_2:.4f}")
print(f"- High Risk: ≥ {threshold_2:.4f}")


# Vectorize the 'Summary' column using TF-IDF
tfidf_vectorizer = TfidfVectorizer(stop_words='english')
tfidf_matrix = tfidf_vectorizer.fit_transform(df['Summary'].fillna(''))

def get_vectorized_summary(query, column):
    """
    Function to summarize numerical insights based on a query.
    Example query: 'What is the average risk factor?'
    """
    if column in df.columns:
        if query.lower() == "average":
            return df[column].mean()
        elif query.lower() == "maximum":
            return df[column].max()
        elif query.lower() == "minimum":
            return df[column].min()
        else:
            raise ValueError("Invalid query for summarization.")
    else:
        raise ValueError(f"Column '{column}' not found in the dataset.")

# Example: Summarizing the average risk factor
average_risk_factor = get_vectorized_summary("average", "Computed Risk Factor")
print(f"Average Risk Factor: {average_risk_factor}")

# Example: Summarizing the maximum risk factor
max_risk_factor = get_vectorized_summary("maximum", "Computed Risk Factor")
print(f"Maximum Risk Factor: {max_risk_factor}")

def query_summary(query_type, column_name):
    """
    Generalized function to handle queries about numerical columns.
    """
    try:
        result = get_vectorized_summary(query_type.lower(), column_name)
        print(f"The {query_type} value for '{column_name}' is: {result}")
    except ValueError as e:
        print(f"Error: {str(e)}")

# Example Queries
query_summary("average", "Computed Risk Factor")
query_summary("maximum", "Inventory Level")
query_summary("minimum", "Lead Time (days)")