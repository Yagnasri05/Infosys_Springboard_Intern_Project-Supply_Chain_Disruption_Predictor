# -*- coding: utf-8 -*-
"""Risk Factorization & Summarization.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wmafdhjV3I97DH_135POtw6qcrcM708g
"""

!pip install transformers torch

import pandas as pd

# Load the dataset
file_path = "/content/drive/MyDrive/merged_supply_chain_data.csv"
data = pd.read_csv(file_path)

# Display the first few rows
print(data.head())

"""# Risk Factorization and Summarization on Entire Data"""

import pandas as pd
from transformers import pipeline
from sklearn.preprocessing import MinMaxScaler
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np


data_path = "/content/drive/MyDrive/merged_supply_chain_data.csv"
df = pd.read_csv(data_path)


numerical_cols = ['Inventory Level', 'Lead Time (days)', 'News Sentiment', 'Risk Factor']

scaler = MinMaxScaler()
df[numerical_cols] = scaler.fit_transform(df[numerical_cols])


# Load pre-trained Hugging Face sentiment model
sentiment_pipeline = pipeline("sentiment-analysis", model="distilbert-base-uncased")

# Generate risk factor from textual data
def calculate_textual_risk(summary):
    sentiment = sentiment_pipeline(summary[:512])  # Limit summary to 512 characters
    sentiment_score = sentiment[0]['score']
    sentiment_label = sentiment[0]['label']
    textual_risk = sentiment_score if sentiment_label == "NEGATIVE" else -sentiment_score
    return textual_risk

# Apply the textual risk factor calculation
df['Textual Risk'] = df['Summary'].apply(calculate_textual_risk)

# Combine numerical and textual data for final risk factor
df['Computed Risk Factor'] = df.apply(
    lambda row: (0.5 * row['Textual Risk'] +
                 0.2 * row['Inventory Level'] +
                 0.2 * row['Lead Time (days)'] +
                 0.1 * row['News Sentiment']), axis=1
)

# Assign Risk Labels
def assign_risk_label(risk_factor):
    if risk_factor >= 0.6:
        return 'High'
    elif risk_factor >= 0.3:
        return 'Medium'
    else:
        return 'Low'

df['Risk Label'] = df['Computed Risk Factor'].apply(assign_risk_label)

# Save the updated dataset
df.to_csv("/content/drive/MyDrive/risk_factorized_data.csv", index=False)
print("Risk factorized data saved to '/content/drive/MyDrive/risk_factorized_data.csv'.")

# Vectorize numerical columns for summarization
vectorizer = TfidfVectorizer()

def get_vectorized_summary(query, column):
    """
    Function to summarize numerical insights based on a query.
    Example query: 'What is the average risk factor?'
    """
    if column in df.columns:
        if query.lower() == "average":
            return df[column].mean()
        elif query.lower() == "maximum":
            return df[column].max()
        elif query.lower() == "minimum":
            return df[column].min()
        else:
            raise ValueError("Invalid query for summarization.")
    else:
        raise ValueError(f"Column '{column}' not found in the dataset.")

# Example: Summarizing the average risk factor
average_risk_factor = get_vectorized_summary("average", "Computed Risk Factor")
print(f"Average Risk Factor: {average_risk_factor}")

# Example: Summarizing the maximum risk factor
max_risk_factor = get_vectorized_summary("maximum", "Computed Risk Factor")
print(f"Maximum Risk Factor: {max_risk_factor}")


def query_summary(query_type, column_name):
    """
    Generalized function to handle queries about numerical columns.
    """
    try:
        result = get_vectorized_summary(query_type.lower(), column_name)
        print(f"The {query_type} value for '{column_name}' is: {result}")
    except ValueError as e:
        print(f"Error: {str(e)}")

# Example Queries
query_summary("average", "Computed Risk Factor")
query_summary("maximum", "Inventory Level")
query_summary("minimum", "Lead Time (days)")

"""# Using Roberta and BART"""

import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from transformers import pipeline

data_path = "/content/drive/MyDrive/merged_supply_chain_data.csv"
df = pd.read_csv(data_path)

# Normalize numerical columns
numerical_cols = ['Inventory Level', 'Lead Time (days)', 'News Sentiment', 'Risk Factor']
scaler = MinMaxScaler()
df[numerical_cols] = scaler.fit_transform(df[numerical_cols])


# Load a sequence classification model (e.g., RoBERTa) for risk factorization
classification_pipeline = pipeline("text-classification", model="roberta-base", return_all_scores=True)

def classify_risk(summary):
    """
    Use a classification model to assign a risk score and label based on the summary.
    """
    try:
        result = classification_pipeline(summary[:512])  # Limit input to 512 tokens
        risk_scores = {item['label']: item['score'] for item in result[0]}
        # Define a custom scoring system
        risk_factor = risk_scores.get("LABEL_0", 0) * 0.2 + risk_scores.get("LABEL_1", 0) * 0.8
        risk_label = "High" if risk_factor > 0.6 else "Medium" if risk_factor > 0.3 else "Low"
        return risk_factor, risk_label
    except Exception as e:
        print(f"Error during classification: {e}")
        return 0.0, "Low"  # Default values in case of error

# Apply classification to the 'Summary' column
df['Risk Factor'] = df['Summary'].apply(lambda x: classify_risk(x)[0])
df['Risk Label'] = df['Summary'].apply(lambda x: classify_risk(x)[1])

# Save the updated dataset
df.to_csv("/content/drive/MyDrive/risk_factorized_data_transformers.csv", index=False)
print("Risk factorized data saved to '/content/drive/MyDrive/risk_factorized_data_transformers.csv'.")

# Load a pre-trained summarization model (e.g., BART)
summarization_pipeline = pipeline("summarization", model="facebook/bart-large-cnn")

def summarize_column(summary):
    """
    Use a summarization model to summarize the textual content.
    """
    try:
        result = summarization_pipeline(summary[:1024], max_length=130, min_length=30, do_sample=False)
        return result[0]['summary_text']
    except Exception as e:
        print(f"Error during summarization: {e}")
        return summary  # Return original summary if summarization fails

# Apply summarization to the 'Summary' column
df['Summarized Content'] = df['Summary'].apply(summarize_column)

# Save summarized data
df.to_csv("/content/drive/MyDrive/summarized_supply_chain_data_transformers.csv", index=False)
print("Summarized data saved to '/content/drive/MyDrive/summarized_supply_chain_data_transformers.csv'.")


def dynamic_query_summary(query, summarization_model):
    """
    Generate a dynamic summary based on the user query.
    """
    try:
        response = summarization_model(query, max_length=100, min_length=30, do_sample=False)
        return response[0]['summary_text']
    except Exception as e:
        print(f"Error generating dynamic summary: {e}")
        return "Unable to process the query."

# Example dynamic query
query = "Summarize the key supply chain risks identified in the dataset."
dynamic_summary = dynamic_query_summary(query, summarization_pipeline)
print(f"Dynamic Summary: {dynamic_summary}")


# Aggregate results by region and supplier
aggregated_data = df.groupby(['Region', 'Supplier']).agg({
    'Risk Factor': 'mean',
    'Inventory Level': 'sum',
    'Lead Time (days)': 'mean',
    'News Sentiment': 'mean',
}).reset_index()

# Save the aggregated results
aggregated_data.to_csv("/content/aggregated_supply_chain_risks_transformers.csv", index=False)
print("Aggregated data saved to '/content/aggregated_supply_chain_risks_transformers.csv'.")

# Display aggregated results
print(aggregated_data.head())