# -*- coding: utf-8 -*-
"""Hugging Face Models Training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MgJJPkHCC5ANAubINxPOKAvEbtvxVZnz
"""

!pip install transformers torch
!pip install Datasets

import pandas as pd

# Load the dataset
file_path = "/content/drive/MyDrive/merged_supply_chain_data.csv"
data = pd.read_csv(file_path)

# Display the first few rows
print(data.head())

from sklearn.preprocessing import MinMaxScaler
from transformers import BertTokenizer

# Tokenizer for textual data
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# Text columns to tokenize
text_columns = ['Keyword', 'Title', 'Summary', 'Transport Status']

# Tokenization function
def tokenize_text(row):
    tokens = {}
    for col in text_columns:
        tokenized = tokenizer(row[col], padding='max_length', truncation=True, max_length=128, return_tensors="pt")
        tokens[col] = tokenized
    return tokens

# Numerical preprocessing
numerical_columns = ['Inventory Level', 'Lead Time (days)', 'News Sentiment', 'Risk Factor']
scaler = MinMaxScaler()
data[numerical_columns] = scaler.fit_transform(data[numerical_columns])

# Apply tokenization
data['tokenized'] = data.apply(tokenize_text, axis=1)

from transformers import BertForSequenceClassification, Trainer, TrainingArguments
from sklearn.model_selection import train_test_split
# Add Risk_Label to the original DataFrame
data['Risk_Label'] = data['Risk Factor'].apply(lambda x: 0 if x < 0.3 else (1 if x < 0.7 else 2))

# Split data into train and test sets
train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)

print(data.columns)
print(train_data.columns)
print(test_data.columns)

"""# BERT"""

import torch
from transformers import BertTokenizer, BertForSequenceClassification
from torch.utils.data import DataLoader, TensorDataset
from torch.optim import AdamW
from sklearn.metrics import classification_report

# Load Hugging Face tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# Tokenize and prepare inputs for BERT
def tokenize_data(texts, sentiments, labels):
    encodings = tokenizer(texts, padding=True, truncation=True, max_length=128, return_tensors="pt")
    sentiment_tensor = torch.tensor(sentiments).unsqueeze(1)  # Add dimension for concatenation if needed
    labels_tensor = torch.tensor(labels)
    return encodings, sentiment_tensor, labels_tensor

# Prepare train data
train_texts = train_data['Title'].tolist()
train_sentiments = train_data['News Sentiment'].tolist()
train_labels = train_data['Risk_Label'].tolist()

train_encodings, train_sentiments_tensor, train_labels_tensor = tokenize_data(train_texts, train_sentiments, train_labels)

# Prepare test data
test_texts = test_data['Title'].tolist()
test_sentiments = test_data['News Sentiment'].tolist()
test_labels = test_data['Risk_Label'].tolist()

test_encodings, test_sentiments_tensor, test_labels_tensor = tokenize_data(test_texts, test_sentiments, test_labels)

# Combine tokenized data into TensorDataset
train_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], train_labels_tensor)
test_dataset = TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'], test_labels_tensor)

# DataLoader for batching
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=16)

# Initialize BERT model
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)  # 3 risk levels
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
model.to(device)

# Define optimizer and loss function
optimizer = AdamW(model.parameters(), lr=2e-5)
loss_fn = torch.nn.CrossEntropyLoss()

# Training loop
epochs = 50
for epoch in range(epochs):
    model.train()
    total_loss = 0
    for batch in train_loader:
        input_ids, attention_mask, labels = [x.to(device) for x in batch]

        # Forward pass
        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        total_loss += loss.item()

        # Backward pass
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

    print(f"Epoch {epoch + 1} Loss: {total_loss / len(train_loader)}")

# Save the model
model.save_pretrained('/content/supply_chain_risk_model')
tokenizer.save_pretrained('/content/supply_chain_risk_model')

# Testing the model
model.eval()
all_preds = []
all_labels = []

with torch.no_grad():
    for batch in test_loader:
        input_ids, attention_mask, labels = [x.to(device) for x in batch]

        outputs = model(input_ids, attention_mask=attention_mask)
        preds = torch.argmax(outputs.logits, dim=1)

        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

# Generate classification report
print("Classification Report:")
print(classification_report(all_labels, all_preds, target_names=['Low Risk', 'Medium Risk', 'High Risk']))

"""# ROBERTA"""

from transformers import RobertaForSequenceClassification, RobertaTokenizer

# Load RoBERTa tokenizer
tokenizer = RobertaTokenizer.from_pretrained('roberta-base')

# Define the model
model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=3)  # 3 risk levels
model.to(device)

# Add Risk_Label to the original DataFrame
data['Risk_Label'] = data['Risk Factor'].apply(lambda x: 0 if x < 0.3 else (1 if x < 0.7 else 2))

# Split data into train and test sets
train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)

import torch
from transformers import BertTokenizer, BertForSequenceClassification
from torch.utils.data import DataLoader, TensorDataset
from torch.optim import AdamW
from sklearn.metrics import classification_report

# Load Hugging Face tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# Tokenize and prepare inputs for BERT
def tokenize_data(texts, sentiments, labels):
    encodings = tokenizer(texts, padding=True, truncation=True, max_length=128, return_tensors="pt")
    sentiment_tensor = torch.tensor(sentiments).unsqueeze(1)  # Add dimension for concatenation if needed
    labels_tensor = torch.tensor(labels)
    return encodings, sentiment_tensor, labels_tensor

# Prepare train data
train_texts = train_data['Title'].tolist()
train_sentiments = train_data['News Sentiment'].tolist()
train_labels = train_data['Risk_Label'].tolist()

train_encodings, train_sentiments_tensor, train_labels_tensor = tokenize_data(train_texts, train_sentiments, train_labels)

# Prepare test data
test_texts = test_data['Title'].tolist()
test_sentiments = test_data['News Sentiment'].tolist()
test_labels = test_data['Risk_Label'].tolist()

test_encodings, test_sentiments_tensor, test_labels_tensor = tokenize_data(test_texts, test_sentiments, test_labels)

# Combine tokenized data into TensorDataset
train_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], train_labels_tensor)
test_dataset = TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'], test_labels_tensor)

# DataLoader for batching
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=16)

# Initialize BERT model
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)  # 3 risk levels
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
model.to(device)

# Define optimizer and loss function
optimizer = AdamW(model.parameters(), lr=2e-5)
loss_fn = torch.nn.CrossEntropyLoss()

# Training loop
epochs = 50
for epoch in range(epochs):
    model.train()
    total_loss = 0
    for batch in train_loader:
        input_ids, attention_mask, labels = [x.to(device) for x in batch]

        # Forward pass
        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        total_loss += loss.item()

        # Backward pass
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

    print(f"Epoch {epoch + 1} Loss: {total_loss / len(train_loader)}")

# Save the model
model.save_pretrained('/content/supply_chain_risk_model')
tokenizer.save_pretrained('/content/supply_chain_risk_model')

# Testing the model
model.eval()
all_preds = []
all_labels = []

with torch.no_grad():
    for batch in test_loader:
        input_ids, attention_mask, labels = [x.to(device) for x in batch]

        outputs = model(input_ids, attention_mask=attention_mask)
        preds = torch.argmax(outputs.logits, dim=1)

        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

# Generate classification report
print("Classification Report:")
print(classification_report(all_labels, all_preds, target_names=['Low Risk', 'Medium Risk', 'High Risk']))

"""# DISTIL BERT"""

from transformers import DistilBertForSequenceClassification, DistilBertTokenizer

# Load DistilBERT tokenizer
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')

# Define the model
model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=3)
model.to(device)

# Add Risk_Label to the original DataFrame
data['Risk_Label'] = data['Risk Factor'].apply(lambda x: 0 if x < 0.3 else (1 if x < 0.7 else 2))

# Split data into train and test sets
train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)

import torch
from transformers import BertTokenizer, BertForSequenceClassification
from torch.utils.data import DataLoader, TensorDataset
from torch.optim import AdamW
from sklearn.metrics import classification_report

# Load Hugging Face tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# Tokenize and prepare inputs for BERT
def tokenize_data(texts, sentiments, labels):
    encodings = tokenizer(texts, padding=True, truncation=True, max_length=128, return_tensors="pt")
    sentiment_tensor = torch.tensor(sentiments).unsqueeze(1)  # Add dimension for concatenation if needed
    labels_tensor = torch.tensor(labels)
    return encodings, sentiment_tensor, labels_tensor

# Prepare train data
train_texts = train_data['Title'].tolist()
train_sentiments = train_data['News Sentiment'].tolist()
train_labels = train_data['Risk_Label'].tolist()

train_encodings, train_sentiments_tensor, train_labels_tensor = tokenize_data(train_texts, train_sentiments, train_labels)

# Prepare test data
test_texts = test_data['Title'].tolist()
test_sentiments = test_data['News Sentiment'].tolist()
test_labels = test_data['Risk_Label'].tolist()

test_encodings, test_sentiments_tensor, test_labels_tensor = tokenize_data(test_texts, test_sentiments, test_labels)

# Combine tokenized data into TensorDataset
train_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], train_labels_tensor)
test_dataset = TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'], test_labels_tensor)

# DataLoader for batching
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=16)

# Initialize BERT model
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)  # 3 risk levels
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
model.to(device)

# Define optimizer and loss function
optimizer = AdamW(model.parameters(), lr=2e-5)
loss_fn = torch.nn.CrossEntropyLoss()

# Training loop
epochs = 50
for epoch in range(epochs):
    model.train()
    total_loss = 0
    for batch in train_loader:
        input_ids, attention_mask, labels = [x.to(device) for x in batch]

        # Forward pass
        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        total_loss += loss.item()

        # Backward pass
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

    print(f"Epoch {epoch + 1} Loss: {total_loss / len(train_loader)}")

# Save the model
model.save_pretrained('/content/supply_chain_risk_model')
tokenizer.save_pretrained('/content/supply_chain_risk_model')

# Testing the model
model.eval()
all_preds = []
all_labels = []

with torch.no_grad():
    for batch in test_loader:
        input_ids, attention_mask, labels = [x.to(device) for x in batch]

        outputs = model(input_ids, attention_mask=attention_mask)
        preds = torch.argmax(outputs.logits, dim=1)

        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

# Generate classification report
print("Classification Report:")
print(classification_report(all_labels, all_preds, target_names=['Low Risk', 'Medium Risk', 'High Risk']))